---
title: "Occam's Razor in <br> Bayesian Model Selection"
author: "<strong> A Short Tutorial </strong>"
date: 'Instructions: <ul><li>Use the arrow keys on the keyboard to move between chapters
  in the notes. </li><li> The full list of chapters can be navigated by clicking on
  ''Contents'' in the bottom left-hand corner. </li><li> Longer chapters require you
  to scroll to see more details. <br><br><br><br> <strong> Kevin Smith | Homework
  3 | Big Data Analytics </strong>'
output:
  slidy_presentation:
    fig_height: 4
    fig_width: 7
    self_contained: no
  ioslides_presentation:
    fig_height: 2
    fig_width: 7
  beamer_presentation: default
---

```{r, echo = FALSE, warning = FALSE, message = FALSE}
require(knitr)
```

Outline
=======================================================
 * Premise
 * Prior Beliefs about $t$ Given $M_i$
 * Interlude: Visualizations
 * Likelihood of Measuring $t_m$ Given Thermometer Parameters
 * Posterior Likelihood of $t$ Given Thermometer Parameters and Model $M_i$
 * Bayesian Model Selection
 * References


Premise
========================================================
 * A group of researchers have developed a suite of simulation models $M_{1},\, M_{2},\,...\, M_{n}$ to predict the temperature of a water bath during a chemical process. 
 * A sensitivity analysis on the models suggests that the predicted temperatures from model $M_i$ are normally distributed about $t_{p,i}$ with standard deviation $\sigma_{p,i}$.
 
 <div style = "text-align: center;">
$M_{i}\equiv t\sim N(t_{p,i},\,\sigma_{p,i})$
</div>

 * The team then takes a direct measurment of the bath. The thermometer is calibrated to remove systematic error. 
 * It is assumed that the remaining measurement error is normally distributed with standard deviation $\sigma_{m}$ about the true value $t$. 

<div style = "text-align: center;">
 $t_{m}\sim N(t,\,\sigma_{m})$
</div>

 * Our goal is to use Bayesian inference to determine which model is the most probable given the data and our beliefs in the thermometer parameters.

Prior Beliefs about $t$ Given $M_{i}$
========================================================
 * Recall that temperatures generated by $M_i$ are normally distributed about $t_m$ with standard devaition $\sigma_{p,i}$:
   <div style = "text-align: center;">$M_{i}\equiv t\sim N(t_{p,i},\,\sigma_{p,i})$ </div>
   <br>
 * Prior Belief About $t$ under $M_i$:
  <div style = "text-align: center;"> $p(t|M_{i},\, t_{p,i},\,\sigma_{p,i})=\frac{1}{\sigma_{p,i\cdot\sqrt{(2\pi)}}}exp[\frac{(-t-t_{i})^{2}}{2\sigma_{p,i}^{2}}]$ </div>

__Example__: Assume for model $M_1$ the researchers report $t_{p,1} = 30$ and $\sigma_{p,1} = 5$
 
In _R_:
```{r}
predicted.mean.1 = 30
predicted.sd.1 = 5

prior.M1 <- function(temperature){ 
  dnorm(temperature, 
        mean = predicted.mean.1, 
        sd = predicted.sd.1) 
}
```

Interlude: Visualizations 
======================================================== 
Plot the prior beliefs for model $M_1$: 
```{r, message=FALSE, warning=FALSE}
require(ggplot2)
```
Create a range of temperatures to evaluate:
```{r}
range <- data.frame(temperature = seq(0, 55, length.out = 200))
```
Create the base layer:
```{r}
g <- ggplot()
```
Add a layer for the prior bliefs about $t$ under model $M_1$: 
```{r}
g <- g + 
  geom_ribbon(data = range,
          aes(x = temperature, ymin = 0, 
              ymax = prior.M1(temperature)), 
          fill = "orange", alpha = 0.6)
```
Add label layer and render the graphics:
```{r, message=FALSE, warning=FALSE}
g <- g + geom_text(aes(x = 10, y = 0.06, 
                  label = "Prior 1"),
                  color = "orange", size = 8) + ylab("Likelihood")
g
```

Likelihood of Measuring $t_m$ Given Thermometer Parameters
========================================================
What is the likelihood of measuring $t_{m}$ assuming normally distributed measurement errors with standard deviation $\sigma_{m}$ about the true value $t$? 
<br>
<br>
Recall: 

<div style = "text-align: center;"> $t_{m}\sim N(t,\,\sigma_{m})$ </div>


The probability density function for the measurement $t_m$ given the true value $t$ and thermometer parameters is:

<div style = "text-align: center;"> $p(t_{m}|t,\,\sigma_{m})=\frac{1}{\sigma_{m\cdot\sqrt{2\pi}}}exp[\frac{(-t_{m}-t)^{2}}{2\sigma_{m}^{2}}]$ </div>

Since only one measurement was taken, the likelihood function of the true parameter $t$ given the measured temperature $t_m$ and the thermometer parameters $\sigma_t$ are equivalent:

<div style = "text-align: center;"> $L(t|t_{m},\,\sigma_{m})=p(t_{m}|t,\,\sigma_{m})$ </div>

__Example__: Assume the measured temperature of the bath $t = 38$ and that the thermometer has $\sigma_{m} = 1$:

```{r}
sample.temperature = 38
measurement.error = 1
```

The probability of obtaining a temperature measurement $t_m$ from a bath of temperature $t$ can be written equivalently as the likelihood of obtaining the true temperature $t$ from an instrument that measures $t_m$ with $\sigma_{m} = 1$:

```{r}
likelihood <- function(temperature){ 
  dnorm(temperature, mean = sample.temperature, sd = measurement.error) 
}
```

The plot from the preceeding page can be updated:
```{r}
g <- g + 
  geom_ribbon(data = range,
          aes(x = temperature, ymin = 0, 
              ymax = likelihood(temperature)), 
          fill = "red", alpha = 0.6) +
   geom_text(aes(x = 50, y = 0.24, 
                  label = "Likelihood"),
                  color = "red", size = 8)

g
```

Posterior Likelihood of $t$ Given Thermometer Parameters and Model $M_{i}$
============================================
___Query___: Given both our __prior__ beliefs ($M_1$) and the __likelihood__ of
obtaining the observed data $t_{m}$, what is the probability the
temperature of the bath is $t$?

 * By Bayes' Rule, the __unnormalized posterior__ belief $\tilde{P}$ about $t$ is the product of the __prior__ and the __likelihood.__ That is:

<div style = "text-align: center"> $\tilde{P}\equiv p(t|M_{i},\, t_{p,i},\,\sigma_{p,i},\, t_{m},\,\sigma_{m})\propto p(t|M_{i},\, t_{p,i},\,\sigma_{p,i})\cdot p(t_{m}|t,\,\sigma_{m})$ </div>

 * Recall that the prior and the likelihood are both normal:

<div style = "text-align: center">
$p(t|M_{i},\, t_{p,i},\,\sigma_{p,i},\, t_{m},\,\sigma_{m})\propto N(t_{p,i},\,\sigma_{p,i}) \cdot N(t,\,\sigma_{m})$ 
</div>


 * So the __unnormalized posterior belief__ $\tilde{P}$ about $t$ is also normal:

<div style = "text-align: center">
$t\sim N(t_{c},\,\sigma_{c})$
</div>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; where $t_{c}$ is the is the most probable value of $t$ under $\tilde{P}$,
given by:

<div style = "text-align: center">
$t_{c}=(\frac{t_{p,i}}{\sigma_{p,i}^{2}}+\frac{t_{m}}{\sigma_{m}^{2}})(\frac{1}{\sigma_{p,i}^{2}}+\frac{1}{\sigma_{m}^{2}})^{-1}$
</div>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; and $\sigma_{c}$ is the standard deviation of $t$ under $\tilde{P}$,
given by:

<div style = "text-align: center">
$\sigma_{c}=(\frac{1}{\sigma_{p,i}^{2}}+\frac{1}{\sigma_{m}^{2}})^{-\frac{1}{2}}$
</div>

__Example:__

 * The posterior can be written in _R_:
 
```{r}
posterior <- function(temperature, prior, scale.factor){ 
  scale.factor * prior(temperature) * likelihood(temperature)
}
```

 * It can be added to the plot like so:
```{r}
g <- g + 
  geom_ribbon(data = range,
          aes(x = temperature, ymin = 0, 
              ymax = posterior(temperature, prior.M1, 30)), 
          fill = "green", alpha = 0.4) + 
  geom_text(aes(x = 50, y = 0.12, 
          label = "Posterior 1"),
          color = "darkgreen", size = 8)
g
```

Bayesian Model Selection
============================================
So only one set of prior beliefs, captured in $M_1$, $t_{p,1}$ and $\sigma_{p,1}$, has been considered. How can multiple sets of prior beliefs be compared?

__Example:__ Assume for model $M_2$ the researchers report $t_{p,2} = 32$ and $\sigma_{p,2} = 2$
 
In _R_:
```{r}
predicted.mean.2 = 32
predicted.sd.2 = 2

prior.M2 <- function(temperature){ 
  dnorm(temperature, 
        mean = predicted.mean.2, 
        sd = predicted.sd.2) 
}
```


 * The prior and posterior based on $M_2$ and its parameters can be added to the plot like so:

```{r}
g <- g + 
  geom_ribbon(data = range,
          aes(x = temperature, ymin = 0, 
              ymax = prior.M2(temperature)), 
          fill = "darkred", alpha = 0.4) + 
  geom_text(aes(x = 25, y = 0.22, 
          label = "Prior 2"),
          color = "darkred", size = 8) + 
  geom_ribbon(data = range,
          aes(x = temperature, ymin = 0, 
              ymax = posterior(temperature, prior.M2, 90)), 
          fill = "darkblue", alpha = 0.4) + 
  geom_text(aes(x = 50, y = 0.06, 
          label = "Posterior 2"),
          color = "darkblue", size = 8)
g
```
 
_N.B._ Remember that the posteriors are not normalized, so their respective maximum values cannot be compared directly.

 * The plot is getting a bit cluttered, but a close examination will reveal that as $\sigma_{p, i}$ decreases, the most probable posterior value of $t_{c, i}$ shifts towards the most probable value of the prior $t_{p, i}$. 
 
 * More precisely since $\sigma_{p, 2} \lt \sigma_{p, 1}$, it follows that $t_{p, 2}$ is closer to $t_{c, 2}$ than $t_{p, 1}$ is to $t_{c, 1}$. However it does __not__ follow that a smaller $\sigma_{p, i}$ results in a _better_ model.
 
 * Indeed, compared to $M_2$, $M_1$ is more adept at predicting $t$ in the probable range of the data $t_m$, given the thermometer parameters. Qualitatively, this is shown by the overlap of the Likelihood function to Priors 1 and 2. For a quantitative assessment, the probability of obtaining data $t_m$ under $M_i$ should be compared for each $i$.
 
 * This is given by the integral of the integral of the marginal joint probability of $t_m$ and $t$ given $M_i$, $t_{p, i}$, $\sigma_{p, i}$, and $\sigma_m$ over all possible values of $t$. That is:
 
 <div style = "text-align: center">
 $p(t_m | M_i, t_{p, i}, \sigma_{p, i}, \sigma_m) = \int_{-\infty}^{\infty} p(t_m, t | M_i, t_{p, i}, \sigma_{p, i}, \sigma_m) \cdot dt$
 </div>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; which can be written:
<div style = "text-align: center">
$p(t_m | M_i, t_{p, i}, \sigma_{p, i}, \sigma_m) = \int_{-\infty}^{\infty} p(t_m | t, \sigma_m) \cdot p(t | M_i, t_{p, i}, \sigma_{p, i}) \cdot dt$
</div>

 * In terms of previously defined functions this is:
<div style = "text-align: center">
$p(t_m | M_i, t_{p, i}, \sigma_{p, i}, \sigma_m) = \int_{-\infty}^{\infty} Likelihood \cdot Prior_i \cdot dt$.  </div>
 
 * In our example, since the Likelihood and Prior are both Gaussian, the following result is obtained:
<div style = "text-align: center">
$p(t_m | M_i, t_{p, i}, \sigma_{p, i}, \sigma_m) = \frac{1}{\sqrt{2\pi(\sigma_m^2 + \sigma_{p, i}^2)}} \cdot exp(- \frac{(t_m - t_{p, i})^2}{2(\sigma_m^2 + \sigma_{p, i}^2)})$.  </div>

 * The factor $\frac{(t_m - t_{p, i})^2}{(\sigma_m^2 + \sigma_{p, i}^2)}$ is a measure of the model's _inaccuracy_. Increasing (↑) the variability in model output $\sigma_{p, i}$ leads to a decrease (↓) in the _inaccuracy_ of prediction and in turn an increase (↑) in the overall probability of measuring $t_m$. 
 
 * On the other hand, the factor $\frac{1}{\sqrt(\sigma_m^2 + \sigma_{p, i}^2)}$ is a measure of the model's _precision_. Increasing (↑) the variability in model output $\sigma_{p, i}$ leads to a  decrease (↓) in the _precision_ of prediction and in turn a decrease (↓) in the overall probability of measuring $t_m$. 
 
 * Therefore, Bayesian model comparison inherently invokes a trade-off between a model's precision and its ability to generalize. According to some authors, (e.g. __Leroy 1998__ and __MacKay 1992__), this is the embodiment of Occam's razor: 
 
 > Non sunt multiplicanda entia sine necessitate.
 <br>
 > 'Entities must not be multiplied beyond necessity.' - Occam's razor. 

 * In this case, we have no prior preference for models $M_1$ or $M_2$. That is $p(M_i) = constant$. The matter of which model is preferable is then simply a matter of which model gives the highest likelihood for measuring $t_m$. This is usually presented as a ratio known as __Bayes factor__, and denoted $K$:
 
<div style = "text-align: center">
 $K = \frac{p(t_m | M_1)}{p(t_m | M_2)}$
</div>

 * __Jeffery's 1961__ gives the following suggestions regarding the strength of the evidence for $M_1$ that can be obtained from $K$. Note that these measures do not explicitly account for _sample size_, so it should be assumed that they are only approximately valid for large sample sizes. 

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
</style>
<table class="tg" align = "center">
  <tr>
    <th class="tg-031e">K</th>
    <th class="tg-031e">Strength of Evidence for $M_1$</th>
  </tr>
  <tr>
    <td class="tg-031e">&lt; 1</td>
    <td class="tg-031e">Negative </td>
  </tr>
  <tr>
    <td class="tg-031e">1 - 3</td>
    <td class="tg-031e">Barely Worth Mentioning</td>
  </tr>
  <tr>
    <td class="tg-031e">3 - 10</td>
    <td class="tg-031e">Substantial</td>
  </tr>
  <tr>
    <td class="tg-031e">10 - 30</td>
    <td class="tg-031e">Strong</td>
  </tr>
  <tr>
    <td class="tg-031e">30 - 100</td>
    <td class="tg-031e">Very Strong<br></td>
  </tr>
  <tr>
    <td class="tg-031e">&gt; 100</td>
    <td class="tg-031e">Decisive</td>
  </tr>
</table>

```{r}

evidence.for.m1 = dnorm(sample.temperature, 
                        mean = predicted.mean.1,
                        sd = measurement.error + predicted.sd.1
                        )

evidence.for.m2 = dnorm(sample.temperature, 
                        mean = predicted.mean.2,
                        sd = measurement.error + predicted.sd.2
                        )

K = evidence.for.m1 / evidence.for.m2
```

```{r, results = 'asis', echo = FALSE}
cat(paste("The value for K in our example is ", round(K, 2), ". So $M_1$ is slightly preferred over $M_2$ even though $M_2$ is more precise. However the strength of the evidence is 'barely worth mentioning' so we will end our analysis here.", sep = ""))
```

References
================================

 * D. J. C. MacKay, “Bayesian interpolation,” Neural computation, vol. 4, no. 3, pp. 415–447, 1992.

 * S. S. Leroy, “Detecting climate signals: Some Bayesian aspects,” Journal of climate, vol. 11, no. 4, pp. 640–651, 1998.

 * H. Jeffreys, Theory of probability, 3rd ed. Oxford [Oxfordshire]: New York: Clarendon Press; Oxford University Press, 1998.


Reproducibility Information
================================

```{r, echo = FALSE, message=FALSE}
require(pander)
pander(sessionInfo())
```



