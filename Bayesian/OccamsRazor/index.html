<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content=" A Short Tutorial" />
  <title>Occam’s Razor in  Bayesian Model Selection</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; }
code > span.dt { color: #902000; }
code > span.dv { color: #40a070; }
code > span.bn { color: #40a070; }
code > span.fl { color: #40a070; }
code > span.ch { color: #4070a0; }
code > span.st { color: #4070a0; }
code > span.co { color: #60a0b0; font-style: italic; }
code > span.ot { color: #007020; }
code > span.al { color: #ff0000; font-weight: bold; }
code > span.fu { color: #06287e; }
code > span.er { color: #ff0000; font-weight: bold; }
  </style>
  <link rel="stylesheet" type="text/css" media="screen, projection, print"
    href="index_files/Slidy2/styles/slidy.css" />
  <script src="index_files/Slidy2/scripts/slidy.js"
    charset="utf-8" type="text/javascript"></script>
</head>
<body>
<div class="slide titlepage">
  <h1 class="title">Occam’s Razor in <br> Bayesian Model Selection</h1>
  <p class="author">
<strong> A Short Tutorial </strong>
  </p>
  <p class="date">Instructions:
<ul><li>
Use the arrow keys on the keyboard to move between chapters in the notes.
</li><li> 
The full list of chapters can be navigated by clicking on ‘Contents’ in the bottom left-hand corner.
</li><li> 
<p>Longer chapters require you to scroll to see more details. <br><br><br><br> <strong> Kevin Smith | Homework 3 | Big Data Analytics </strong></p></p>
</div>
<div id="outline" class="slide section level1">
<h1>Outline</h1>
<ul>
<li>Premise</li>
<li>Prior Beliefs about <span class="math">\(t\)</span> Given <span class="math">\(M_i\)</span></li>
<li>Interlude: Visualizations</li>
<li>Likelihood of Measuring <span class="math">\(t_m\)</span> Given Thermometer Parameters</li>
<li>Posterior Likelihood of <span class="math">\(t\)</span> Given Thermometer Parameters and Model <span class="math">\(M_i\)</span></li>
<li>Bayesian Model Selection</li>
<li>References</li>
</ul>
</div>
<div id="premise" class="slide section level1">
<h1>Premise</h1>
<ul>
<li>A group of researchers have developed a suite of simulation models <span class="math">\(M_{1},\, M_{2},\,...\, M_{n}\)</span> to predict the temperature of a water bath during a chemical process.</li>
<li>A sensitivity analysis on the models suggests that the predicted temperatures from model <span class="math">\(M_i\)</span> are normally distributed about <span class="math">\(t_{p,i}\)</span> with standard deviation <span class="math">\(\sigma_{p,i}\)</span>.</li>
</ul>
<div style="text-align: center;">
<span class="math">\(M_{i}\equiv t\sim N(t_{p,i},\,\sigma_{p,i})\)</span>
</div>
<ul>
<li>The team then takes a direct measurment of the bath. The thermometer is calibrated to remove systematic error.</li>
<li>It is assumed that the remaining measurement error is normally distributed with standard deviation <span class="math">\(\sigma_{m}\)</span> about the true value <span class="math">\(t\)</span>.</li>
</ul>
<div style="text-align: center;">
<span class="math">\(t_{m}\sim N(t,\,\sigma_{m})\)</span>
</div>
<ul>
<li>Our goal is to use Bayesian inference to determine which model is the most probable given the data and our beliefs in the thermometer parameters.</li>
</ul>
</div>
<div id="prior-beliefs-about-t-given-m_i" class="slide section level1">
<h1>Prior Beliefs about <span class="math">\(t\)</span> Given <span class="math">\(M_{i}\)</span></h1>
<ul>
<li>Recall that temperatures generated by <span class="math">\(M_i\)</span> are normally distributed about <span class="math">\(t_m\)</span> with standard devaition <span class="math">\(\sigma_{p,i}\)</span>:
<div style="text-align: center;">
<span class="math">\(M_{i}\equiv t\sim N(t_{p,i},\,\sigma_{p,i})\)</span>
</div>
<br></li>
<li>Prior Belief About <span class="math">\(t\)</span> under <span class="math">\(M_i\)</span>:
<div style="text-align: center;">
<span class="math">\(p(t|M_{i},\, t_{p,i},\,\sigma_{p,i})=\frac{1}{\sigma_{p,i\cdot\sqrt{(2\pi)}}}exp[\frac{(-t-t_{i})^{2}}{2\sigma_{p,i}^{2}}]\)</span>
</div></li>
</ul>
<p><strong>Example</strong>: Assume for model <span class="math">\(M_1\)</span> the researchers report <span class="math">\(t_{p,1} = 30\)</span> and <span class="math">\(\sigma_{p,1} = 5\)</span></p>
<p>In <em>R</em>:</p>
<pre class="sourceCode r"><code class="sourceCode r">predicted.mean<span class="fl">.1</span> =<span class="st"> </span><span class="dv">30</span>
predicted.sd<span class="fl">.1</span> =<span class="st"> </span><span class="dv">5</span>

prior.M1 &lt;-<span class="st"> </span>function(temperature){ 
  <span class="kw">dnorm</span>(temperature, 
        <span class="dt">mean =</span> predicted.mean<span class="fl">.1</span>, 
        <span class="dt">sd =</span> predicted.sd<span class="fl">.1</span>) 
}</code></pre>
</div>
<div id="interlude-visualizations" class="slide section level1">
<h1>Interlude: Visualizations</h1>
<p>Plot the prior beliefs for model <span class="math">\(M_1\)</span>:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(ggplot2)</code></pre>
<p>Create a range of temperatures to evaluate:</p>
<pre class="sourceCode r"><code class="sourceCode r">range &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">temperature =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">55</span>, <span class="dt">length.out =</span> <span class="dv">200</span>))</code></pre>
<p>Create the base layer:</p>
<pre class="sourceCode r"><code class="sourceCode r">g &lt;-<span class="st"> </span><span class="kw">ggplot</span>()</code></pre>
<p>Add a layer for the prior bliefs about <span class="math">\(t\)</span> under model <span class="math">\(M_1\)</span>:</p>
<pre class="sourceCode r"><code class="sourceCode r">g &lt;-<span class="st"> </span>g +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_ribbon</span>(<span class="dt">data =</span> range,
          <span class="kw">aes</span>(<span class="dt">x =</span> temperature, <span class="dt">ymin =</span> <span class="dv">0</span>, 
              <span class="dt">ymax =</span> <span class="kw">prior.M1</span>(temperature)), 
          <span class="dt">fill =</span> <span class="st">&quot;orange&quot;</span>, <span class="dt">alpha =</span> <span class="fl">0.6</span>)</code></pre>
<p>Add label layer and render the graphics:</p>
<pre class="sourceCode r"><code class="sourceCode r">g &lt;-<span class="st"> </span>g +<span class="st"> </span><span class="kw">geom_text</span>(<span class="kw">aes</span>(<span class="dt">x =</span> <span class="dv">10</span>, <span class="dt">y =</span> <span class="fl">0.06</span>, 
                  <span class="dt">label =</span> <span class="st">&quot;Prior 1&quot;</span>),
                  <span class="dt">color =</span> <span class="st">&quot;orange&quot;</span>, <span class="dt">size =</span> <span class="dv">8</span>) +<span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;Likelihood&quot;</span>)
g</code></pre>
<p><img src="index_files/figure-slidy/unnamed-chunk-7-1.png" title="" alt="" width="672" /></p>
</div>
<div id="likelihood-of-measuring-t_m-given-thermometer-parameters" class="slide section level1">
<h1>Likelihood of Measuring <span class="math">\(t_m\)</span> Given Thermometer Parameters</h1>
<p>What is the likelihood of measuring <span class="math">\(t_{m}\)</span> assuming normally distributed measurement errors with standard deviation <span class="math">\(\sigma_{m}\)</span> about the true value <span class="math">\(t\)</span>? <br> <br> Recall:</p>
<div style="text-align: center;">
<span class="math">\(t_{m}\sim N(t,\,\sigma_{m})\)</span>
</div>
<p>The probability density function for the measurement <span class="math">\(t_m\)</span> given the true value <span class="math">\(t\)</span> and thermometer parameters is:</p>
<div style="text-align: center;">
<span class="math">\(p(t_{m}|t,\,\sigma_{m})=\frac{1}{\sigma_{m\cdot\sqrt{2\pi}}}exp[\frac{(-t_{m}-t)^{2}}{2\sigma_{m}^{2}}]\)</span>
</div>
<p>Since only one measurement was taken, the likelihood function of the true parameter <span class="math">\(t\)</span> given the measured temperature <span class="math">\(t_m\)</span> and the thermometer parameters <span class="math">\(\sigma_t\)</span> are equivalent:</p>
<div style="text-align: center;">
<span class="math">\(L(t|t_{m},\,\sigma_{m})=p(t_{m}|t,\,\sigma_{m})\)</span>
</div>
<p><strong>Example</strong>: Assume the measured temperature of the bath <span class="math">\(t = 38\)</span> and that the thermometer has <span class="math">\(\sigma_{m} = 1\)</span>:</p>
<pre class="sourceCode r"><code class="sourceCode r">sample.temperature =<span class="st"> </span><span class="dv">38</span>
measurement.error =<span class="st"> </span><span class="dv">1</span></code></pre>
<p>The probability of obtaining a temperature measurement <span class="math">\(t_m\)</span> from a bath of temperature <span class="math">\(t\)</span> can be written equivalently as the likelihood of obtaining the true temperature <span class="math">\(t\)</span> from an instrument that measures <span class="math">\(t_m\)</span> with <span class="math">\(\sigma_{m} = 1\)</span>:</p>
<pre class="sourceCode r"><code class="sourceCode r">likelihood &lt;-<span class="st"> </span>function(temperature){ 
  <span class="kw">dnorm</span>(temperature, <span class="dt">mean =</span> sample.temperature, <span class="dt">sd =</span> measurement.error) 
}</code></pre>
<p>The plot from the preceeding page can be updated:</p>
<pre class="sourceCode r"><code class="sourceCode r">g &lt;-<span class="st"> </span>g +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_ribbon</span>(<span class="dt">data =</span> range,
          <span class="kw">aes</span>(<span class="dt">x =</span> temperature, <span class="dt">ymin =</span> <span class="dv">0</span>, 
              <span class="dt">ymax =</span> <span class="kw">likelihood</span>(temperature)), 
          <span class="dt">fill =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">alpha =</span> <span class="fl">0.6</span>) +
<span class="st">   </span><span class="kw">geom_text</span>(<span class="kw">aes</span>(<span class="dt">x =</span> <span class="dv">50</span>, <span class="dt">y =</span> <span class="fl">0.24</span>, 
                  <span class="dt">label =</span> <span class="st">&quot;Likelihood&quot;</span>),
                  <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">size =</span> <span class="dv">8</span>)

g</code></pre>
<p><img src="index_files/figure-slidy/unnamed-chunk-10-1.png" title="" alt="" width="672" /></p>
</div>
<div id="posterior-likelihood-of-t-given-thermometer-parameters-and-model-m_i" class="slide section level1">
<h1>Posterior Likelihood of <span class="math">\(t\)</span> Given Thermometer Parameters and Model <span class="math">\(M_{i}\)</span></h1>
<p><strong><em>Query</em></strong>: Given both our <strong>prior</strong> beliefs (<span class="math">\(M_1\)</span>) and the <strong>likelihood</strong> of obtaining the observed data <span class="math">\(t_{m}\)</span>, what is the probability the temperature of the bath is <span class="math">\(t\)</span>?</p>
<ul>
<li>By Bayes’ Rule, the <strong>unnormalized posterior</strong> belief <span class="math">\(\tilde{P}\)</span> about <span class="math">\(t\)</span> is the product of the <strong>prior</strong> and the <strong>likelihood.</strong> That is:</li>
</ul>
<div style="text-align: center">
<span class="math">\(\tilde{P}\equiv p(t|M_{i},\, t_{p,i},\,\sigma_{p,i},\, t_{m},\,\sigma_{m})\propto p(t|M_{i},\, t_{p,i},\,\sigma_{p,i})\cdot p(t_{m}|t,\,\sigma_{m})\)</span>
</div>
<ul>
<li>Recall that the prior and the likelihood are both normal:</li>
</ul>
<div style="text-align: center">
<span class="math">\(p(t|M_{i},\, t_{p,i},\,\sigma_{p,i},\, t_{m},\,\sigma_{m})\propto N(t_{p,i},\,\sigma_{p,i}) \cdot N(t,\,\sigma_{m})\)</span>
</div>
<ul>
<li>So the <strong>unnormalized posterior belief</strong> <span class="math">\(\tilde{P}\)</span> about <span class="math">\(t\)</span> is also normal:</li>
</ul>
<div style="text-align: center">
<span class="math">\(t\sim N(t_{c},\,\sigma_{c})\)</span>
</div>
<p>      where <span class="math">\(t_{c}\)</span> is the is the most probable value of <span class="math">\(t\)</span> under <span class="math">\(\tilde{P}\)</span>, given by:</p>
<div style="text-align: center">
<span class="math">\(t_{c}=(\frac{t_{p,i}}{\sigma_{p,i}^{2}}+\frac{t_{m}}{\sigma_{m}^{2}})(\frac{1}{\sigma_{p,i}^{2}}+\frac{1}{\sigma_{m}^{2}})^{-1}\)</span>
</div>
<p>      and <span class="math">\(\sigma_{c}\)</span> is the standard deviation of <span class="math">\(t\)</span> under <span class="math">\(\tilde{P}\)</span>, given by:</p>
<div style="text-align: center">
<span class="math">\(\sigma_{c}=(\frac{1}{\sigma_{p,i}^{2}}+\frac{1}{\sigma_{m}^{2}})^{-\frac{1}{2}}\)</span>
</div>
<p><strong>Example:</strong></p>
<ul>
<li>The posterior can be written in <em>R</em>:</li>
</ul>
<pre class="sourceCode r"><code class="sourceCode r">posterior &lt;-<span class="st"> </span>function(temperature, prior, scale.factor){ 
  scale.factor *<span class="st"> </span><span class="kw">prior</span>(temperature) *<span class="st"> </span><span class="kw">likelihood</span>(temperature)
}</code></pre>
<ul>
<li>It can be added to the plot like so:</li>
</ul>
<pre class="sourceCode r"><code class="sourceCode r">g &lt;-<span class="st"> </span>g +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_ribbon</span>(<span class="dt">data =</span> range,
          <span class="kw">aes</span>(<span class="dt">x =</span> temperature, <span class="dt">ymin =</span> <span class="dv">0</span>, 
              <span class="dt">ymax =</span> <span class="kw">posterior</span>(temperature, prior.M1, <span class="dv">30</span>)), 
          <span class="dt">fill =</span> <span class="st">&quot;green&quot;</span>, <span class="dt">alpha =</span> <span class="fl">0.4</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_text</span>(<span class="kw">aes</span>(<span class="dt">x =</span> <span class="dv">50</span>, <span class="dt">y =</span> <span class="fl">0.12</span>, 
          <span class="dt">label =</span> <span class="st">&quot;Posterior 1&quot;</span>),
          <span class="dt">color =</span> <span class="st">&quot;darkgreen&quot;</span>, <span class="dt">size =</span> <span class="dv">8</span>)
g</code></pre>
<p><img src="index_files/figure-slidy/unnamed-chunk-12-1.png" title="" alt="" width="672" /></p>
</div>
<div id="bayesian-model-selection" class="slide section level1">
<h1>Bayesian Model Selection</h1>
<p>So only one set of prior beliefs, captured in <span class="math">\(M_1\)</span>, <span class="math">\(t_{p,1}\)</span> and <span class="math">\(\sigma_{p,1}\)</span>, has been considered. How can multiple sets of prior beliefs be compared?</p>
<p><strong>Example:</strong> Assume for model <span class="math">\(M_2\)</span> the researchers report <span class="math">\(t_{p,2} = 32\)</span> and <span class="math">\(\sigma_{p,2} = 2\)</span></p>
<p>In <em>R</em>:</p>
<pre class="sourceCode r"><code class="sourceCode r">predicted.mean<span class="fl">.2</span> =<span class="st"> </span><span class="dv">32</span>
predicted.sd<span class="fl">.2</span> =<span class="st"> </span><span class="dv">2</span>

prior.M2 &lt;-<span class="st"> </span>function(temperature){ 
  <span class="kw">dnorm</span>(temperature, 
        <span class="dt">mean =</span> predicted.mean<span class="fl">.2</span>, 
        <span class="dt">sd =</span> predicted.sd<span class="fl">.2</span>) 
}</code></pre>
<ul>
<li>The prior and posterior based on <span class="math">\(M_2\)</span> and its parameters can be added to the plot like so:</li>
</ul>
<pre class="sourceCode r"><code class="sourceCode r">g &lt;-<span class="st"> </span>g +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_ribbon</span>(<span class="dt">data =</span> range,
          <span class="kw">aes</span>(<span class="dt">x =</span> temperature, <span class="dt">ymin =</span> <span class="dv">0</span>, 
              <span class="dt">ymax =</span> <span class="kw">prior.M2</span>(temperature)), 
          <span class="dt">fill =</span> <span class="st">&quot;darkred&quot;</span>, <span class="dt">alpha =</span> <span class="fl">0.4</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_text</span>(<span class="kw">aes</span>(<span class="dt">x =</span> <span class="dv">25</span>, <span class="dt">y =</span> <span class="fl">0.22</span>, 
          <span class="dt">label =</span> <span class="st">&quot;Prior 2&quot;</span>),
          <span class="dt">color =</span> <span class="st">&quot;darkred&quot;</span>, <span class="dt">size =</span> <span class="dv">8</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_ribbon</span>(<span class="dt">data =</span> range,
          <span class="kw">aes</span>(<span class="dt">x =</span> temperature, <span class="dt">ymin =</span> <span class="dv">0</span>, 
              <span class="dt">ymax =</span> <span class="kw">posterior</span>(temperature, prior.M2, <span class="dv">90</span>)), 
          <span class="dt">fill =</span> <span class="st">&quot;darkblue&quot;</span>, <span class="dt">alpha =</span> <span class="fl">0.4</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_text</span>(<span class="kw">aes</span>(<span class="dt">x =</span> <span class="dv">50</span>, <span class="dt">y =</span> <span class="fl">0.06</span>, 
          <span class="dt">label =</span> <span class="st">&quot;Posterior 2&quot;</span>),
          <span class="dt">color =</span> <span class="st">&quot;darkblue&quot;</span>, <span class="dt">size =</span> <span class="dv">8</span>)
g</code></pre>
<p><img src="index_files/figure-slidy/unnamed-chunk-14-1.png" title="" alt="" width="672" /></p>
<p><em>N.B.</em> Remember that the posteriors are not normalized, so their respective maximum values cannot be compared directly.</p>
<ul>
<li><p>The plot is getting a bit cluttered, but a close examination will reveal that as <span class="math">\(\sigma_{p, i}\)</span> decreases, the most probable posterior value of <span class="math">\(t_{c, i}\)</span> shifts towards the most probable value of the prior <span class="math">\(t_{p, i}\)</span>.</p></li>
<li><p>More precisely since <span class="math">\(\sigma_{p, 2} \lt \sigma_{p, 1}\)</span>, it follows that <span class="math">\(t_{p, 2}\)</span> is closer to <span class="math">\(t_{c, 2}\)</span> than <span class="math">\(t_{p, 1}\)</span> is to <span class="math">\(t_{c, 1}\)</span>. However it does <strong>not</strong> follow that a smaller <span class="math">\(\sigma_{p, i}\)</span> results in a <em>better</em> model.</p></li>
<li><p>Indeed, compared to <span class="math">\(M_2\)</span>, <span class="math">\(M_1\)</span> is more adept at predicting <span class="math">\(t\)</span> in the probable range of the data <span class="math">\(t_m\)</span>, given the thermometer parameters. Qualitatively, this is shown by the overlap of the Likelihood function to Priors 1 and 2. For a quantitative assessment, the probability of obtaining data <span class="math">\(t_m\)</span> under <span class="math">\(M_i\)</span> should be compared for each <span class="math">\(i\)</span>.</p></li>
<li><p>This is given by the integral of the integral of the marginal joint probability of <span class="math">\(t_m\)</span> and <span class="math">\(t\)</span> given <span class="math">\(M_i\)</span>, <span class="math">\(t_{p, i}\)</span>, <span class="math">\(\sigma_{p, i}\)</span>, and <span class="math">\(\sigma_m\)</span> over all possible values of <span class="math">\(t\)</span>. That is:</p></li>
</ul>
<div style="text-align: center">
<span class="math">\(p(t_m | M_i, t_{p, i}, \sigma_{p, i}, \sigma_m) = \int_{-\infty}^{\infty} p(t_m, t | M_i, t_{p, i}, \sigma_{p, i}, \sigma_m) \cdot dt\)</span>
</div>
      which can be written:
<div style="text-align: center">
<span class="math">\(p(t_m | M_i, t_{p, i}, \sigma_{p, i}, \sigma_m) = \int_{-\infty}^{\infty} p(t_m | t, \sigma_m) \cdot p(t | M_i, t_{p, i}, \sigma_{p, i}) \cdot dt\)</span>
</div>
<ul>
<li>In terms of previously defined functions this is:
<div style="text-align: center">
<span class="math">\(p(t_m | M_i, t_{p, i}, \sigma_{p, i}, \sigma_m) = \int_{-\infty}^{\infty} Likelihood \cdot Prior_i \cdot dt\)</span>.
</div></li>
<li>In our example, since the Likelihood and Prior are both Gaussian, the following result is obtained:
<div style="text-align: center">
<span class="math">\(p(t_m | M_i, t_{p, i}, \sigma_{p, i}, \sigma_m) = \frac{1}{\sqrt{2\pi(\sigma_m^2 + \sigma_{p, i}^2)}} \cdot exp(- \frac{(t_m - t_{p, i})^2}{2(\sigma_m^2 + \sigma_{p, i}^2)})\)</span>.
</div></li>
<li><p>The factor <span class="math">\(\frac{(t_m - t_{p, i})^2}{(\sigma_m^2 + \sigma_{p, i}^2)}\)</span> is a measure of the model’s <em>inaccuracy</em>. Increasing (↑) the variability in model output <span class="math">\(\sigma_{p, i}\)</span> leads to a decrease (↓) in the <em>inaccuracy</em> of prediction and in turn an increase (↑) in the overall probability of measuring <span class="math">\(t_m\)</span>.</p></li>
<li><p>On the other hand, the factor <span class="math">\(\frac{1}{\sqrt(\sigma_m^2 + \sigma_{p, i}^2)}\)</span> is a measure of the model’s <em>precision</em>. Increasing (↑) the variability in model output <span class="math">\(\sigma_{p, i}\)</span> leads to a decrease (↓) in the <em>precision</em> of prediction and in turn a decrease (↓) in the overall probability of measuring <span class="math">\(t_m\)</span>.</p></li>
<li><p>Therefore, Bayesian model comparison inherently invokes a trade-off between a model’s precision and its ability to generalize. According to some authors, (e.g. <strong>Leroy 1998</strong> and <strong>MacKay 1992</strong>), this is the embodiment of Occam’s razor:</p></li>
</ul>
<blockquote>
<p>Non sunt multiplicanda entia sine necessitate. <br> ‘Entities must not be multiplied beyond necessity.’ - Occam’s razor.</p>
</blockquote>
<ul>
<li>In this case, we have no prior preference for models <span class="math">\(M_1\)</span> or <span class="math">\(M_2\)</span>. That is <span class="math">\(p(M_i) = constant\)</span>. The matter of which model is preferable is then simply a matter of which model gives the highest likelihood for measuring <span class="math">\(t_m\)</span>. This is usually presented as a ratio known as <strong>Bayes factor</strong>, and denoted <span class="math">\(K\)</span>:</li>
</ul>
<div style="text-align: center">
<span class="math">\(K = \frac{p(t_m | M_1)}{p(t_m | M_2)}\)</span>
</div>
<ul>
<li><strong>Jeffery’s 1961</strong> gives the following suggestions regarding the strength of the evidence for <span class="math">\(M_1\)</span> that can be obtained from <span class="math">\(K\)</span>. Note that these measures do not explicitly account for <em>sample size</em>, so it should be assumed that they are only approximately valid for large sample sizes.</li>
</ul>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
</style>
<table class="tg" align = "center">
  <tr>
    <th class="tg-031e">
K
</th>
    <th class="tg-031e">
Strength of Evidence for <span class="math">\(M_1\)</span>
</th>
  </tr>
  <tr>
    <td class="tg-031e">
&lt; 1
</td>
    <td class="tg-031e">
Negative
</td>
  </tr>
  <tr>
    <td class="tg-031e">
1 - 3
</td>
    <td class="tg-031e">
Barely Worth Mentioning
</td>
  </tr>
  <tr>
    <td class="tg-031e">
3 - 10
</td>
    <td class="tg-031e">
Substantial
</td>
  </tr>
  <tr>
    <td class="tg-031e">
10 - 30
</td>
    <td class="tg-031e">
Strong
</td>
  </tr>
  <tr>
    <td class="tg-031e">
30 - 100
</td>
    <td class="tg-031e">
Very Strong<br>
</td>
  </tr>
  <tr>
    <td class="tg-031e">
&gt; 100
</td>
    <td class="tg-031e">
Decisive
</td>
  </tr>
</table>


<pre class="sourceCode r"><code class="sourceCode r">evidence.for.m1 =<span class="st"> </span><span class="kw">dnorm</span>(sample.temperature, 
                        <span class="dt">mean =</span> predicted.mean<span class="fl">.1</span>,
                        <span class="dt">sd =</span> measurement.error +<span class="st"> </span>predicted.sd<span class="fl">.1</span>
                        )

evidence.for.m2 =<span class="st"> </span><span class="kw">dnorm</span>(sample.temperature, 
                        <span class="dt">mean =</span> predicted.mean<span class="fl">.2</span>,
                        <span class="dt">sd =</span> measurement.error +<span class="st"> </span>predicted.sd<span class="fl">.2</span>
                        )

K =<span class="st"> </span>evidence.for.m1 /<span class="st"> </span>evidence.for.m2</code></pre>
<p>The value for K in our example is 1.52. So <span class="math">\(M_1\)</span> is slightly preferred over <span class="math">\(M_2\)</span> even though <span class="math">\(M_2\)</span> is more precise. However the strength of the evidence is ‘barely worth mentioning’ so we will end our analysis here.</p>
</div>
<div id="references" class="slide section level1">
<h1>References</h1>
<ul>
<li><p>D. J. C. MacKay, “Bayesian interpolation,” Neural computation, vol. 4, no. 3, pp. 415–447, 1992.</p></li>
<li><p>S. S. Leroy, “Detecting climate signals: Some Bayesian aspects,” Journal of climate, vol. 11, no. 4, pp. 640–651, 1998.</p></li>
<li><p>H. Jeffreys, Theory of probability, 3rd ed. Oxford [Oxfordshire]: New York: Clarendon Press; Oxford University Press, 1998.</p></li>
</ul>
</div>
<div id="reproducibility-information" class="slide section level1">
<h1>Reproducibility Information</h1>
<p><strong>R version 3.1.2 (2014-10-31)</strong></p>
<p><strong>Platform:</strong> x86_64-apple-darwin13.4.0 (64-bit)</p>
<p><strong>locale:</strong></p>
<p><strong>attached base packages:</strong> [1] “<em>stats</em>, <em>graphics</em>, <em>grDevices</em>, <em>utils</em>, <em>datasets</em>, <em>methods</em> and <em>base</em>” attr(,“class”) [1] “knit_asis” attr(,“knit_cacheable”) [1] TRUE</p>
<p><strong>other attached packages:</strong> [1] “<em>pander(v.0.5.1)</em>, <em>ggplot2(v.1.0.0)</em> and <em>knitr(v.1.8)</em>” attr(,“class”) [1] “knit_asis” attr(,“knit_cacheable”) [1] TRUE</p>
<p><strong>loaded via a namespace (and not attached):</strong> [1] “<em>colorspace(v.1.2-4)</em>, <em>digest(v.0.6.4)</em>, <em>evaluate(v.0.5.5)</em>, <em>formatR(v.1.0)</em>, <em>grid(v.3.1.2)</em>, <em>gtable(v.0.1.2)</em>, <em>htmltools(v.0.2.6)</em>, <em>labeling(v.0.3)</em>, <em>MASS(v.7.3-35)</em>, <em>munsell(v.0.4.2)</em>, <em>plyr(v.1.8.1)</em>, <em>proto(v.0.3-10)</em>, <em>Rcpp(v.0.11.3)</em>, <em>reshape2(v.1.4.1)</em>, <em>rmarkdown(v.0.3.10)</em>, <em>scales(v.0.2.4)</em>, <em>stringr(v.0.6.2)</em>, <em>tools(v.3.1.2)</em> and <em>yaml(v.2.1.13)</em>” attr(,“class”) [1] “knit_asis” attr(,“knit_cacheable”) [1] TRUE</p>
</div>

  <!-- dynamically load mathjax for compatibility with self-contained -->
  <script>
    (function () {
      var script = document.createElement("script");
      script.type = "text/javascript";
      script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
      document.getElementsByTagName("head")[0].appendChild(script);
    })();
  </script>

</body>
</html>
